<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="google-site-verification" content="DykGAn8-hiUCZwQR_LqQ1V-Rg8UOhvXmI32I8HTY81s" />
<!--<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />-->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<!--<meta  name="viewport" content="width=device-width, initial-scale=1.0, minimum-scale=0.5, maximum-scale=2.0, user-scalable=yes"/>-->
<meta http-equiv="Content-Type" content="text/html; charset=gb2312"/>
<link rel="stylesheet" href="Files/jemdoc.css" type="text/css" />

<link rel="shortcut icon" href="./Files/favicon.ico">
<title>Zhuo Su</title>
</head>
 


<body> 




<a id="home" class="anchor"></a>
<div id="container"> 
<div class="container"> 

<h1>Zhuo Su 苏 卓</h1></A>
<table class="imgtable"><tr><td>
<a href="./"><img src="./Files/zhousu1.gif" alt="" height="200px" /></a>&nbsp;</td>
<td align="left"><p><a href="./"></a><br />
<i> I am working in GY-Lab, Tencent, as a senior researcher / engineer. I received my Master degree from Department of Automation, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, supervised by <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html">Prof. Qionghai Dai</a> and
    <a href="http://luvision.net/">Prof. Lu Fang</a>, and meanwhile, I worked closely with <a href="https://liuyebin.com/">Prof. Yebin Liu</a> and <a href="https://www.xu-lan.com/index.html">Lan Xu</a>. My research focuses on computer vision
    and graphics, especially human performance capture, 3D reconstruction and so on.
</a></i>
<br />
<br />
Email: su-z18@tsinghua.org.cn / suzhuo13@gmail.com 
<br />
<br />
<class="staffshortcut">
 <A HREF="#Background">Background</A> | 
 <A HREF="#Interests">Interests</A> | 
 <A HREF="#Research">Research</A> | 
 <A HREF="#Awards">Awards</A>|
 <A HREF="#Skills">Skills</A>
 <!--<a href="./Files/cv_zhuosu.pdf">CV</a>-->
<br />
<br />
 

</td></tr></table>


 

<A NAME="Background"><h2>Background</h2></A>
<ul>
    <font style="line-height:1.8;">
        <b>M.S.</b>,&nbsp Department of Automation, <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>, Beijing, China.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2018.08-2021.06 &nbsp;&nbsp;&nbsp;&nbsp;
        <br />
        GPA: 3.89/4.0 (GPA ranking: 5/137)
        <br />
        <b>B.E.</b>,&nbsp Department of Automation, <a href="http://english.neu.edu.cn/">Northeastern University</a>, Shenyang, China.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; 2014.09-2018.06 &nbsp;&nbsp;&nbsp;&nbsp;
        <br />
        GPA: 4.18/5.0 (GPA ranking: 5/276; Comprehensive ranking: 1/276)
    </font>
</span></ul>
<br />


 
<A NAME="Interests"><h2>Interests</h2></A>
    &nbsp;&nbsp;&nbsp;&nbsp; My research interests include computer vision and comptuter graphics. Currently, I focus on the following topics:
<ul>
<li>Human Performance Capture</li>
<Li>Reconstruction of Dynamic Scenes</Li>
<li>Photorealistic 3D Modeling</li>
</ul>
<br />




<A NAME="Research"><h2>Research</h2></A>
<font size="3"> 
<ul>


<table class="imgtable"><tr><td>
    <a href="./"><img src="./Files/stylemotion.png" alt="" height="134px" /></a>&nbsp;</td>
    <td align="left"><p><a href="./"></a>
    <p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span> 
    <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Learning Variational Motion Prior for Video-based Motion Capture</b>
        <br />
        <b>Zhuo Su*</b>, Xin Chen*</a>, <a> Lingbo Yang*</a>, <a> Pei Cheng</a>, <a> Lan Xu</a>, <a>Gang Yu</a> (* Equal Contribution)
        <b><i>IEEE VR, Under Review.</i></b>
        <br /><br />
        We propose a novel variational motion prior (VMP) learning approach for video-based motion capture. Specifically, VMP is implemented as a transformer-based variational autoencoder pretrained over large-scale 3D motion data, providing an expressive latent space for human motion at sequence level. 
        <br />
        <!--[<a href="./Projects/RobustFusion_plus_page/RobustFusionPlus.pdf">Paper</a>]
        [<a href="./Projects/RobustFusion_plus_page/index.html">Project page</a>]-->
    
    </a></span>
    </p>
    </td></tr></table>    

    <table class="imgtable"><tr><td>
        <a href="./"><img src="./Files/robustfusionplus.jpg" alt="" height="193px" /></a>&nbsp;</td>
        <td align="left"><p><a href="./"></a>
        <p style="text-indent: -1.6rem;margin-left: 0rem;">
        <span> 
        
        <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Robust Volumetric Performance Reconstruction under Human-object Interactions from Monocular RGBD Stream</b>
            <br />
            <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <a>Dawei Zhong</a>, <a>Zhong Li</a>, <a>Fan Deng</a>, <a>Shuxue Quan</a>, <a href="http://luvision.net/">Lu Fang</a>
            <br />
            <b><i>TPAMI 2022, Accepted.</i></b>
            <br /><br />
            <!-- We propose UnstructuredFusion, which allows realtime, high-quality, complete reconstruction of 4D textured models of human performance via only three commercial RGBD cameras. -->
            We propose a robust volumetric performance reconstruction system for human-object interaction scenarios using only a single RGBD sensor, which combines various data-driven visual and interaction cues to handle the complex interaction patterns and severe occlusions. 
            <br />
            [<a href="./Projects/RobustFusion_plus_page/RobustFusionPlus.pdf">Paper</a>]
            [<a href="./Projects/RobustFusion_plus_page/index.html">Project page</a>]
        
        </a></span>
        </p>
        </td></tr></table>



<table class="imgtable"><tr><td>
    <a href="./"><img src="./Files/neuralrendering.jpg" alt="" height="138px" /></a>&nbsp;</td>
    <td align="left"><p><a href="./"></a>
    <p style="text-indent: -1.6rem;margin-left: 0rem;">
    <span> 
    <b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;NeuralFusion: Neural Volumetric Rendering under Human-object Interactions</b>
        <br />
        <!--<b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <a>Dawei Zhong</a>, <a>Zhong Li</a>, <a>Fan Deng</a>, <a>Shuxue Quan</a>, <a href="http://luvision.net/">Lu Fang</a>-->
        <b><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Accepted, 2022.</i></b>
        <br /><br />
        We propose a robust neural volumetric rendering method for human-object interaction scenarios using 6 RGBD cameras, which achieves layer-wise and photorealistic reconstruction results of human performance in novel views.
        <br />
        <!--[<a href="./Projects/RobustFusion_plus_page/RobustFusionPlus.pdf">Paper</a>]
        [<a href="./Projects/RobustFusion_plus_page/index.html">Project page</a>]-->
    
    </a></span>
    </p>
    </td></tr></table>




<table class="imgtable"><tr><td>
<a href="./"><img src="./Files/robustfusion.jpg" alt="" height="180px" /></a>&nbsp;</td>
<td align="left"><p><a href="./"></a>
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span> 

    <b>
        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;RobustFusion: Human Volumetric Capture with Data-driven Visual Cues using a RGBD Camera
        <br />
    </b>
    <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <a href="https://zhengzerong.github.io/">Zerong Zheng</a>, <a href="https://ytrock.com/">Tao Yu</a>, <a href="https://liuyebin.com/">Yebin Liu</a>, <a href="http://luvision.net/">
    Lu Fang</a>
    <br />
    <b><i>European Conference on Computer Vision (ECCV), 2020, Spotlight.</i></b>
    <br /><br />
    We introduce a robust human volumetric capture approach combined with various data-driven visual cues using a Kinect, which outperforms existing state-of-the-art approaches significantly.
    <br />
    [<a href="./Files/Rofusion.pdf">Paper</a>]
    [<a href="./Projects/RobustFusion_page/index.html">Project page</a>]
</a></span>
</p>
</td></tr></table>

<table class="imgtable"><tr><td>
<a href="./"><img src="./Files/unfusion.jpg" alt="" height="150px" /></a>&nbsp;</td>
<td align="left"><p><a href="./"></a>
<p style="text-indent: -1.6rem;margin-left: 0rem;">
<span> 

<b> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;UnstructuredFusion: Realtime 4D Geometry and Texture Reconstruction using Commercial RGBD Cameras</b>
    <br />
    <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, <b>Zhuo Su</b>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <a href="https://ytrock.com/">Tao Yu</a>, <a href="https://liuyebin.com/">Yebin Liu</a>, <a href="http://luvision.net/">Lu Fang</a>
    <br />
    <b><i>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019.</i></b>
    <br /><br />
    We propose UnstructuredFusion, which allows realtime, high-quality, complete reconstruction of 4D textured models of human performance via only three commercial RGBD cameras.
    <br />
    [<a href="./Files/Unfusion.pdf">Paper</a>]
    [<a href="./Projects/UnstructedFusion_page/index.html">Project page</a>]

</a></span>
</p>
</td></tr></table>




</ul>
<br />
 

 

<A NAME="Projects"><h2>Patents & early publications</h2></A>
<font size="3"> 
<ul>
    <li> <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html"> Qionghai Dai</a>,
         “Depth camera calibration method and device, electronic equipment and storage medium”, CN:201810179738:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <a href="https://sites.google.com/view/lhanaf/%E9%A6%96%E9%A1%B5">Lei Han</a>, <b>Zhuo Su</b>, <a href="http://media.au.tsinghua.edu.cn/english/team/qhdai.html"> Qionghai Dai</a>,
        “A three-dimensional rebuilding method and device based on a depth camera, an apparatus and a storage medium”, CN:201810179264:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>,
        “Dynamic three-dimensional reconstruction method, device, equipment, medium and system”, CN:201910110062:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>,
        “Texture real-time determination method, device and equipment for dynamic scene and medium”, CN:201910110044:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, Jianwei Wen, Chao Yuan,
        “Dynamic human body three-dimensional reconstruction method, device, equipment and medium”, CN:202010838902:A
    </li>
    <li>
        <a href="http://luvision.net/">Lu Fang</a>, <b>Zhuo Su</b>, <a href="https://www.xu-lan.com/index.html">Lan Xu</a>, Jianwei Wen, Chao Yuan,
        “Dynamic human body three-dimensional model completion method and device, equipment and medium”, CN:202010838890:A
    </li>
    <li>
        <b>Zhuo Su</b>, Xiaozhe Wang, Wen Fei, Changfu Zhou,
        “Multi-feature information landmark detection method for precise landing of unmanned aerial vehicle”, CCN:201710197369:A
    </li>
    <li> Wen Fei, <b>Zhuo Su*</b> (*corresponding author), Changfu Zhou,
         “Artificial landmark design and detection using hierarchy information for UAV localization and landing”,
    Chinese Control And Decision Conference 2017 (CCDC 2017),
          [<a href="./Files/UAV.pdf">Paper</a>]
    </li>
    <li>Haina Wu, <b>Zhuo Su</b>, Kai Luo, Qi Wang, Xianzhong Cheng
        "Exploration and Research on the Movement of Magnus Glider”, Physical Experiment of College, 2015 (5): 2
        <!--[<a href="./Files/Glider.pdf">Paper</a>]-->
    </li>
</ul>
<br />


<!-- awards -->
<A NAME="Awards"><h2>Awards</h2></A>
<font size="3"> 
<ul>
<li>Outstanding Graduate of Beijing, Beijing, 2021</li>
<li>Outstanding Graduate of Department of Automation, Tsinghua University, 2021</li>
<li>Excellent Bachelor Thesis Award, Northeastern University, 2018</li>
<li>Outstanding Graduate of Liaoning Province, Liaoning Province, 2018</li>
<li>National Scholarship, Ministry of Education, 2018</li>
<li>Excellence Award for National Undergraduate Innovation Program, Northeastern University, 2017</li>
<li>City's Excellent Undergraduate, Shenyang City, 2017</li>
<li>Mayor's Scholarship, Shenyang City, 2017</li>
<li>Top Ten Excellent Undergraduate (10 / the whole university), Northeastern University, 2017</li>
<li>Honorable Mention of American Mathematical Contest in Modeling, COMAP, 2017</li>
<li>Second Prize of National Undergraduate Mathematical Contest in Modeling, CSIAM, 2016</li>
<li>First Prize of Provincial Undergraduate Mathematical Contest in Modeling, Liaoning Province, 2016</li>
<li>2x Second Prize of Electronic Design Contest, Education Department of Liaoning Province, 2015-2016</li>
<li>4x First Class Scholarships, Northeastern University, 2015-2018</li>
</ul>
</font>
<br />

<A NAME="Skills"><h2>Skills</h2></A>
&nbsp;&nbsp;&nbsp;&nbsp; C & C++(OpenCV, OpenGL, CUDA, Eigen, ...), Python(Pytorch), Matlab, LaTeX, ...
<br />
<br />

<!--<script type="text/javascript" src="//rf.revolvermaps.com/0/0/6.js?i=5vmure15sa4&amp;m=7&amp;c=e63100&amp;cr1=ffffff&amp;f=arial&amp;l=0&amp;bv=90&amp;lx=-420&amp;ly=420&amp;hi=20&amp;he=7&amp;hc=a8ddff&amp;rs=80" async="async"></script>
-->
</body>
</html>
